This uses image features to compute the motion of a robot. The first step is to extract meaningful features from images. For every subsequent frame, the movement of features must be tracked in between, and unhelpful associations must filtered out (remove un-helpful outliers, using [[Maths/Probability & Statistics/RANSAC|RANSAC]]). This allows for estimation of optical flow fields, the relative directions in which each of your pixels are moving, which can be also be filtered to remove unhelpful outliers. We can use this to estimate the relative motion in the sensor frame through transformation matrices

Visual SLAM differs from VO by maintaining a history of transformations and ensuring they are all consistent, by optimising them using loop-closure detection.